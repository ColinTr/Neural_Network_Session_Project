<h1 align="center">
  Neural Network Session Project
</h1>

<div align="center">
  
  [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
</div>


## ğŸ” Overview

This project was carried out during the final year of our Master in Computer Science.
Here, we explore the implementation of active learning techniques in neural networks, specifically focusing on classification tasks.
The primary goal was to enhance the learning efficiency by allowing the network to select the most informative data points for labeling, thereby reducing the need for a fully labeled dataset.

For the full context, we recommend the reader to open the <a href="https://github.com/ColinTr/Neural_Network_Session_Project/blob/master/TROISEMAINE_BOUCHARD_INACIO_Rapport_Projet_de_Session_IFT780.pdf">project report</a> (in french).


## ğŸ“‚ Directory Structure

To use train.py and visualisation.py, you need to be in the ./src folder. 

    .
    â”œâ”€â”€ .gitignore
    â”œâ”€â”€ AUTHORS.md                          <- The names of the project's authors
    â”œâ”€â”€ README.md                           <- This file
    â”œâ”€â”€ requirements.txt                    <- The required packages
    â”œâ”€â”€ results                             <- The generated results
    â””â”€â”€ src                                 <- The source code
        â”œâ”€â”€ models                          <- Base class, base blocks and implementation of the models
        â”‚   â”œâ”€â”€ DNNBaseModel.py             <- The base class of which our model's inherit
        â”‚   â”œâ”€â”€ DNNBlocks.py                <- The base blocks which make up our models
        â”‚   â”œâ”€â”€ LeNet.py                    <- The LeNet neural network
        â”‚   â”œâ”€â”€ ResNeXt.py                  <- WIP : The ResNeXt neural network
        â”‚   â””â”€â”€ VGGNet.py                   <- The VGGNet neural network
        â”œâ”€â”€ scenarii                        <- Main scenario and strategies
        â”‚   â”œâ”€â”€ BaseScenario.py             <- The base class of which our scenarios inherit
        â”‚   â””â”€â”€ PoolBaseSamplingScenario.py <- The implementation of the pool sampling scenario and its strategies
        â”œâ”€â”€ scripts                         <- The scripts detailed below
        â”‚   â”œâ”€â”€ launch_calcul_canada.sh     <- Used to launch runner.py on calcul canada
        â”‚   â”œâ”€â”€ mean.py                     <- Used to compute the mean values of all results in a folder
        â”‚   â”œâ”€â”€ mean-visualisation.py       <- Plots the computed mean values
        â”‚   â”œâ”€â”€ runner.py                   <- Launches sequentially train.py instances
        â”‚   â””â”€â”€ visualisation.py            <- Plots the various metrics of a single result file
        â”œâ”€â”€ DataManager.py                  <- The manager of the DataLoader objects
        â”œâ”€â”€ train.py                        <- The main entry point of our project
        â””â”€â”€ TrainTestManager.py             <- The class which hosts the active learning's logic
   
## ğŸ Requirements

This project is meant to be used with python 3.7.


## ğŸ’» Usage

### Training :

The main entry point script is **train.py**. It launches one full active learning loop.
To use it, type :

> cd src (important !)
> 
> python3 scripts/train.py model dataset [parameters]

The parameters are :\
--model : The model to use (values : vggnet / lenet) (default = vggnet).\
--dataset : The dataset to use (values : CIFAR-10 / mnist) (default = cifar10).\
--strategy : The data selection criterion (values : least_confidence / entropy_sampling / margin_sampling / random_sampling) (default = least_confidence).\
--stopping_criterion_value : The quantity of data to add to the seed dataset (default = 100).\
--seed_size : The size of the initial training dataset (default = 100).\
--batch_szie : The batch size used when training the model (default = 20). Reduce this value if you have memory errors.\
--query_size : The quantity of data added to the seed dataset after one iteration of the active learning's loop (default = 20).\
--optimizer : The optimizer used to train the model (values : SGD or Adam) (default = Adam).\
--num_epochs : The number of epochs to train each model (default = 10).\
--validation : The share of data used for the validation dataset (default = 0.1).\
--lr : The learning rate (default = 0.001).

To launch multiple instances of train.py one after the other, edit **runner.py** to fit your needs and simply launch it with :
> cd src
> 
> python scripts/runner.py

### Visualisation :
There are 3 scripts useful for data visualisation.

1) **visualisation.py**

    The first one is visualisation.py. It displays graphs with the following metrics : training and validation losses, training and validation accuracies.\
    To launch it, use :
    > cd src
    > 
    > python scripts/visualisation.py [any number of result files]


2) **mean.py**

    To be able to plot the mean validation accuracy curves, you first need to compute the mean values using the mean.py script.\
    To use it, type :
    > cd src
    > 
    > python scripts/mean.py [folder path]

    It will compute the mean of every result file of the given folder path and write the result inside the folder ./results/mean


3) **mean-visualisation.py**
    Lastly, this script will allow you to plot the mean learning curves with the variance around each curve.
    To use it, type :
    > cd src
    > 
    > python scripts/mean-visualisation.py [any number of **mean** result files]


## âš–ï¸ License

This code is released under the MIT license. See the LICENSE file for more information.
